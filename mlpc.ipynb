{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import wandb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExploration:\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "\n",
    "    def describe_dataset(self, print_all):\n",
    "        if print_all:\n",
    "            print(self.df.describe())\n",
    "        # print(self.df.head())\n",
    "        # print(self.df.info())\n",
    "\n",
    "    def correlation_list(self, printer):\n",
    "        correlationdict = {}        \n",
    "        for feature in self.df.columns:\n",
    "            correlation = self.df[feature].corr(self.df['quality'])\n",
    "            correlationdict[feature] = correlation\n",
    "\n",
    "        average_correlation = {}\n",
    "\n",
    "        for feature, correlation_value in correlationdict.items():\n",
    "            average_correlation[feature] = abs(correlation_value)\n",
    "\n",
    "        sorted_table = sorted(average_correlation.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        headers = [\"Feature\", \"Absolute Correlation\"]\n",
    "        if printer:\n",
    "            print(tabulate(sorted_table, headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDA_Plots(DataExploration):        \n",
    "    def histogram_of_features(self, plotter):\n",
    "        if plotter:\n",
    "            self.df.hist(figsize=(15, 10), bins=1140, edgecolor = \"blue\")\n",
    "            plt.suptitle('How the Numerical Features are Distributed')\n",
    "            plt.show()\n",
    "\n",
    "    def correlation_matrix(self, plotter):\n",
    "        if plotter:\n",
    "            correlation_matrix = self.df.corr()\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "            plt.title('Correlation Matrix')\n",
    "            plt.show()\n",
    "\n",
    "    def pairplot_top_features(self, plotter):\n",
    "        if plotter:\n",
    "            selected_features = ['alcohol', 'volatile acidity', 'sulphates', 'citric acid']\n",
    "            sns.pairplot(self.df, vars=selected_features, hue='quality', palette='tab20', plot_kws={'alpha': 0.5})\n",
    "            plt.suptitle('Pairplot of Selected Features Colored by Genre', y=1.02)\n",
    "            plt.show()    \n",
    "    \n",
    "    def pairplot_all_features(self, plotter):\n",
    "        if plotter:\n",
    "            sns.pairplot(self.df, hue='quality', palette='tab20', plot_kws={'alpha': 0.5})\n",
    "            plt.suptitle('Pairplot of Selected Features Colored by Genre', y=1.02)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplit(EDA_Plots):\n",
    "\n",
    "    def normalize_data(self):\n",
    "        features = self.df.drop(columns=['quality', 'Id'])\n",
    "        labels = self.df['quality']\n",
    "    \n",
    "        ss = StandardScaler()\n",
    "        features_standardized = ss.fit_transform(features)\n",
    "        \n",
    "        normz = MinMaxScaler()\n",
    "        features_normalized = normz.fit_transform(features_standardized)\n",
    "\n",
    "        df_normalized = pd.DataFrame(features_normalized, columns=features.columns)\n",
    "\n",
    "        df_normalized['quality'] = labels.values\n",
    "        df_normalized['Id'] = self.df['Id']\n",
    "\n",
    "        path = 'C:/Users/raaga/OneDrive/Desktop/IIIT-H/3-1/SMAI/smai-m24-assignments-rraagav/data/interim/3/WineQT_Normalized.csv'\n",
    "        df_normalized.to_csv(path, index=False)  \n",
    "\n",
    "        return df_normalized\n",
    "\n",
    "    def split_data(self, data):\n",
    "        features = data.drop(columns=['quality', 'Id'])\n",
    "        # features = data[['alcohol', 'volatile acidity', 'sulphates', 'citric acid']]      \n",
    "        labels = pd.Categorical(data['quality']).codes\n",
    "\n",
    "        # print(\"Original labels:\")\n",
    "        # print(data['quality'].unique())\n",
    "        \n",
    "        # print(\"\\nEncoded labels:\")\n",
    "        # print(np.unique(labels))\n",
    "        \n",
    "        # # Display the distribution of labels\n",
    "        # print('\\nDistribution of the original labels:')\n",
    "        # print(data['quality'].value_counts())\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        indices = np.arange(len(features))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_size = 0.8\n",
    "        train_index = int(len(features) * train_size)\n",
    "\n",
    "        val_size = 0.1\n",
    "        val_index = int(len(features) * val_size)\n",
    "        \n",
    "        train_indices = indices[:train_index]\n",
    "        val_indices = indices[train_index:train_index+val_index]\n",
    "        test_indices = indices[train_index+val_index:]\n",
    "    \n",
    "        X_train = features.iloc[train_indices].values\n",
    "        y_train = labels[train_indices]\n",
    "\n",
    "        X_eval = features.iloc[val_indices].values\n",
    "        y_eval = labels[val_indices]\n",
    "\n",
    "        X_test = features.iloc[test_indices].  values\n",
    "        y_test = labels[test_indices]\n",
    "\n",
    "        return X_train, y_train, X_eval, y_eval, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\raaga\\OneDrive\\Desktop\\IIIT-H\\3-1\\SMAI\\smai-m24-assignments-rraagav\\data\\interim\\3\\WineQT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------------+\n",
      "| Feature              |   Absolute Correlation |\n",
      "+======================+========================+\n",
      "| quality              |              1         |\n",
      "+----------------------+------------------------+\n",
      "| alcohol              |              0.484866  |\n",
      "+----------------------+------------------------+\n",
      "| volatile acidity     |              0.407394  |\n",
      "+----------------------+------------------------+\n",
      "| sulphates            |              0.25771   |\n",
      "+----------------------+------------------------+\n",
      "| citric acid          |              0.240821  |\n",
      "+----------------------+------------------------+\n",
      "| total sulfur dioxide |              0.183339  |\n",
      "+----------------------+------------------------+\n",
      "| density              |              0.175208  |\n",
      "+----------------------+------------------------+\n",
      "| chlorides            |              0.124085  |\n",
      "+----------------------+------------------------+\n",
      "| fixed acidity        |              0.12197   |\n",
      "+----------------------+------------------------+\n",
      "| Id                   |              0.0697082 |\n",
      "+----------------------+------------------------+\n",
      "| free sulfur dioxide  |              0.0632596 |\n",
      "+----------------------+------------------------+\n",
      "| pH                   |              0.052453  |\n",
      "+----------------------+------------------------+\n",
      "| residual sugar       |              0.0220019 |\n",
      "+----------------------+------------------------+\n"
     ]
    }
   ],
   "source": [
    "DataExplorer = DataExploration(df)\n",
    "DataExplorer.describe_dataset(print_all=False)\n",
    "DataExplorer.correlation_list(printer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Explorer_plots = EDA_Plots(df)\n",
    "Explorer_plots.histogram_of_features(plotter=False)\n",
    "Explorer_plots.correlation_matrix(plotter=False)\n",
    "Explorer_plots.pairplot_top_features(plotter=False)\n",
    "Explorer_plots.pairplot_all_features(plotter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = DataSplit(df)\n",
    "df_normalized = splitter.normalize_data()\n",
    "\n",
    "X_train, y_train, X_eval, y_eval, X_test, y_test = splitter.split_data(df_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Classifier:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.alpha = None\n",
    "        self.activation_function = None\n",
    "        self.activation_derivative = None\n",
    "        self.optimizers = None\n",
    "        self.hidden_layers = None\n",
    "        self.neurons_per_layer = None\n",
    "        self.batch_size = None\n",
    "        self.epochs = None\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def set_params(self, alpha, activation_function, optimizer, hidden_layers, neurons_per_layer, batch_size, epochs):\n",
    "        self.alpha = alpha\n",
    "        self.set_activation_function(activation_function)\n",
    "        self.optimizers = optimizer\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        layer_sizes = [self.input_size] + self.neurons_per_layer + [self.output_size]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01 # Xavier initialization, or He \n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        Zs = []\n",
    "        for i in range(len(self.weights)):\n",
    "            w = self.weights[i]\n",
    "            b = self.biases[i]\n",
    "            Z = np.dot(activations[-1], w) + b\n",
    "            Zs.append(Z)\n",
    "            if i == len(self.weights) - 1: \n",
    "                # Output/last layer in nn. Do softmax. \n",
    "                A = self.softmax(Z)\n",
    "            else:\n",
    "                # hidden layer present, use the activation function \n",
    "                A = self.activation_function(Z)\n",
    "            activations.append(A)\n",
    "        return activations, Zs\n",
    "        \n",
    "    def backward(self, activations, Zs, y):\n",
    "        gradients_w = []\n",
    "        gradients_b = []\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        # One-hot encode y\n",
    "        y_one_hot = np.zeros_like(activations[-1])\n",
    "        y_one_hot[np.arange(m), y] = 1\n",
    "        \n",
    "        # Output layer error\n",
    "        delta = activations[-1] - y_one_hot \n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dW = np.dot(activations[i].T, delta) / m  \n",
    "            dB = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            gradients_w.insert(0, dW)\n",
    "            gradients_b.insert(0, dB)\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T)\n",
    "                delta *= self.activation_derivative(Zs[i-1])\n",
    "        return gradients_w, gradients_b\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        sig = self.sigmoid(Z)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    def tanh(self, Z):\n",
    "        return np.tanh(Z)\n",
    "    \n",
    "    def tanh_derivative(self, Z):\n",
    "        return 1 - np.tanh(Z) ** 2\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def relu_derivative(self, Z):\n",
    "        return np.where(Z > 0, 1, 0)\n",
    "    \n",
    "    def linear(self, Z):\n",
    "        return Z\n",
    "    \n",
    "    def linear_derivative(self, Z):\n",
    "        return np.ones_like(Z)\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        exp_scores = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    def set_activation_function(self, func_name):\n",
    "        if func_name == \"sigmoid\":\n",
    "            self.activation_function = self.sigmoid\n",
    "            self.activation_derivative = self.sigmoid_derivative\n",
    "        elif func_name == \"tanh\":\n",
    "            self.activation_function = self.tanh\n",
    "            self.activation_derivative = self.tanh_derivative\n",
    "        elif func_name == \"relu\":\n",
    "            self.activation_function = self.relu\n",
    "            self.activation_derivative = self.relu_derivative\n",
    "        elif func_name == \"linear\":\n",
    "            self.activation_function = self.linear\n",
    "            self.activation_derivative = self.linear_derivative\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(np.log(y_pred[np.arange(m), y_true] + 1e-8)) / m\n",
    "        return loss\n",
    "\n",
    "    def compute_accuracy(self, y_pred, y_true):\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        accuracy = np.mean(y_pred_classes == y_true)\n",
    "        return accuracy\n",
    "        \n",
    "    def compute_classification_metrics(self, y_true, y_pred):\n",
    "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def update_parameters(self, gradients_w, gradients_b):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.alpha * gradients_w[i]\n",
    "            self.biases[i] -= self.alpha * gradients_b[i]\n",
    "    \n",
    "    def bgd(self, X_train, y_train):\n",
    "        activations, Zs = self.forward(X_train)\n",
    "        gradients_w, gradients_b = self.backward(activations, Zs, y_train)\n",
    "        self.update_parameters(gradients_w, gradients_b)\n",
    "        loss = self.compute_loss(activations[-1], y_train)\n",
    "        accuracy = self.compute_accuracy(activations[-1], y_train)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    def mini_bgd(self, X_train, y_train):\n",
    "        m = X_train.shape[0]\n",
    "        perm = np.random.permutation(m)\n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "        num_batches = int(np.ceil(m / self.batch_size))\n",
    "        for i in range(num_batches):\n",
    "            start = i * self.batch_size\n",
    "            end = min(start + self.batch_size, m)\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            activations, Zs = self.forward(X_batch)\n",
    "            gradients_w, gradients_b = self.backward(activations, Zs, y_batch)\n",
    "            self.update_parameters(gradients_w, gradients_b)\n",
    "        activations, _ = self.forward(X_train)\n",
    "        loss = self.compute_loss(activations[-1], y_train)\n",
    "        accuracy = self.compute_accuracy(activations[-1], y_train)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    def sgd(self, X_train, y_train):\n",
    "        m = X_train.shape[0]\n",
    "        perm = np.random.permutation(m)\n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "        for i in range(m):\n",
    "            X_sample = X_shuffled[i:i+1]\n",
    "            y_sample = y_shuffled[i:i+1]\n",
    "            activations, Zs = self.forward(X_sample)\n",
    "            gradients_w, gradients_b = self.backward(activations, Zs, y_sample)\n",
    "            \n",
    "            self.update_parameters(gradients_w, gradients_b)\n",
    "        activations, _ = self.forward(X_train)\n",
    "        loss = self.compute_loss(activations[-1], y_train)\n",
    "        accuracy = self.compute_accuracy(activations[-1], y_train)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    def fit(self, X_train, y_train, early_stopping, patience=5):\n",
    "        m = X_train.shape[0]\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        self.initialize_weights()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            if self.optimizers == 'bgd':\n",
    "                epoch_loss, epoch_accuracy = self.bgd(X_train, y_train)\n",
    "            elif self.optimizers == 'mini_bgd':\n",
    "                epoch_loss, epoch_accuracy = self.mini_bgd(X_train, y_train)\n",
    "            elif self.optimizers == 'sgd':\n",
    "                epoch_loss, epoch_accuracy = self.sgd(X_train, y_train)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid optimizer name\")\n",
    "            \n",
    "            self.losses.append(epoch_loss)\n",
    "            self.accuracies.append(epoch_accuracy)\n",
    "\n",
    "            # Early stopping part\n",
    "            if early_stopping:\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                if patience_counter > patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, Zs = self.forward(X)\n",
    "        y_pred = np.argmax(activations[-1], axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def gradient_check(self, X, y, epsilon=1e-7):\n",
    "        activations, Zs = self.forward(X)\n",
    "        gradients_w, gradients_b = self.backward(activations, Zs, y)\n",
    "        \n",
    "        params = []\n",
    "        grads = []\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            params.extend([w, b])\n",
    "        for gw, gb in zip(gradients_w, gradients_b):\n",
    "            grads.extend([gw, gb])\n",
    "        \n",
    "        num_grads = []\n",
    "        for param in params:\n",
    "            num_grad = np.zeros_like(param)\n",
    "            iterator = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not iterator.finished:\n",
    "                idx = iterator.multi_index\n",
    "                original_value = param[idx]\n",
    "                \n",
    "                param[idx] = original_value + epsilon\n",
    "                activations_plus, _ = self.forward(X)\n",
    "                loss_plus = self.compute_loss(activations_plus[-1], y)\n",
    "                \n",
    "                param[idx] = original_value - epsilon\n",
    "                activations_minus, _ = self.forward(X)\n",
    "                loss_minus = self.compute_loss(activations_minus[-1], y)\n",
    "                \n",
    "                param[idx] = original_value\n",
    "                \n",
    "                num_grad[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "                iterator.iternext()\n",
    "            num_grads.append(num_grad)\n",
    "        \n",
    "        for i in range(len(grads)):\n",
    "            grad = grads[i]\n",
    "            num_grad = num_grads[i]\n",
    "            numerator = np.linalg.norm(grad - num_grad)\n",
    "            denominator = np.linalg.norm(grad) + np.linalg.norm(num_grad) + 1e-8\n",
    "            relative_error = numerator / denominator\n",
    "            print(f\"Parameter {i+1} - Relative Error: {relative_error:.10e}\")\n",
    "            if relative_error > 1e-5:\n",
    "                print(\"Gradients dont match!!!vfgsdgbhjxndwgtbeiurowefnipwmod.\")\n",
    "                return\n",
    "        print(\"It worksssssssssssssssss\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.2297, Accuracy: 0.3950\n",
      "Epoch 2/50, Loss: 1.1871, Accuracy: 0.4256\n",
      "Epoch 3/50, Loss: 1.1812, Accuracy: 0.4311\n",
      "Epoch 4/50, Loss: 1.1793, Accuracy: 0.4792\n",
      "Epoch 5/50, Loss: 1.2052, Accuracy: 0.4322\n",
      "Epoch 6/50, Loss: 1.1607, Accuracy: 0.4956\n",
      "Epoch 7/50, Loss: 1.1447, Accuracy: 0.5011\n",
      "Epoch 8/50, Loss: 1.1430, Accuracy: 0.5066\n",
      "Epoch 9/50, Loss: 1.1439, Accuracy: 0.5077\n",
      "Epoch 10/50, Loss: 1.1513, Accuracy: 0.4639\n",
      "Epoch 11/50, Loss: 1.1366, Accuracy: 0.5000\n",
      "Epoch 12/50, Loss: 1.1379, Accuracy: 0.5000\n",
      "Epoch 13/50, Loss: 1.1348, Accuracy: 0.5120\n",
      "Epoch 14/50, Loss: 1.1436, Accuracy: 0.5011\n",
      "Epoch 15/50, Loss: 1.1370, Accuracy: 0.5098\n",
      "Epoch 16/50, Loss: 1.1460, Accuracy: 0.4956\n",
      "Epoch 17/50, Loss: 1.1407, Accuracy: 0.5131\n",
      "Epoch 18/50, Loss: 1.1472, Accuracy: 0.4989\n",
      "Early stopping at epoch 19\n",
      "Test Loss: 1.0831, Test Accuracy: 0.5391\n",
      "Test Precision: 0.4606, Test Recall: 0.5391, Test F1: 0.4964\n",
      "Train Loss: 1.1399, Train Accuracy: 0.5033\n",
      "Train Precision: 0.4259, Train Recall: 0.5033, Train F1: 0.4563\n",
      "Eval Loss: 1.0899, Eval Accuracy: 0.5263\n",
      "Eval Precision: 0.4877, Eval Recall: 0.5263, Eval F1: 0.4838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_txt_file = open(\"C:/Users/raaga/OneDrive/Desktop/IIIT-H/3-1/SMAI/smai-m24-assignments-rraagav/assignments/3/figures/best_accuracy.txt\", \"w\")\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "output_size = len(np.unique(y_train))\n",
    "\n",
    "mlp_activ = MLP_Classifier(input_size, output_size)\n",
    "\n",
    "mlp_activ.set_params(alpha=0.01, activation_function='tanh', optimizer='sgd', hidden_layers= 3, neurons_per_layer=[128, 64, 32], batch_size=32, epochs=50)\n",
    "mlp_activ.fit(X_train, y_train, early_stopping=True, patience=5)\n",
    "\n",
    "test_activations, _ = mlp_activ.forward(X_test)\n",
    "test_loss = mlp_activ.compute_loss(test_activations[-1], y_test)\n",
    "test_accuracy = mlp_activ.compute_accuracy(test_activations[-1], y_test)\n",
    "test_precision, test_recall, test_f1 = mlp_activ.compute_classification_metrics(y_test, mlp_activ.predict(X_test))\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}\")\n",
    "\n",
    "train_activations, _ = mlp_activ.forward(X_train)\n",
    "train_loss = mlp_activ.compute_loss(train_activations[-1], y_train)\n",
    "train_accuracy = mlp_activ.compute_accuracy(train_activations[-1], y_train)\n",
    "train_precision, train_recall, train_f1 = mlp_activ.compute_classification_metrics(y_train, mlp_activ.predict(X_train))\n",
    "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "eval_activations, _ = mlp_activ.forward(X_eval)\n",
    "eval_loss = mlp_activ.compute_loss(eval_activations[-1], y_eval)\n",
    "eval_accuracy = mlp_activ.compute_accuracy(eval_activations[-1], y_eval)\n",
    "eval_precision, eval_recall, eval_f1 = mlp_activ.compute_classification_metrics(y_eval, mlp_activ.predict(X_eval))\n",
    "print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")\n",
    "print(f\"Eval Precision: {eval_precision:.4f}, Eval Recall: {eval_recall:.4f}, Eval F1: {eval_f1:.4f}\")\n",
    "\n",
    "output_txt_file.write(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "output_txt_file.write(f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}\\n\\n\")\n",
    "\n",
    "output_txt_file.write(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\\n\")\n",
    "output_txt_file.write(f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\\n\\n\")\n",
    "\n",
    "output_txt_file.write(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\\n\")\n",
    "output_txt_file.write(f\"Eval Precision: {eval_precision:.4f}, Eval Recall: {eval_recall:.4f}, Eval F1: {eval_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of non linearity\n",
    "# activ_list = ['sigmoid', 'tanh', 'relu', 'linear']\n",
    "# output_txt_file = open(\"C:/Users/raaga/OneDrive/Desktop/IIIT-H/3-1/SMAI/smai-m24-assignments-rraagav/assignments/3/figures/activation_functions.txt\", \"w\")\n",
    "\n",
    "# input_size = X_train.shape[1]\n",
    "# output_size = len(np.unique(y_train))\n",
    "\n",
    "# mlp_activ = MLP_Classifier(input_size, output_size)\n",
    "\n",
    "# for activ in activ_list:\n",
    "#     mlp_activ.set_params(alpha=0.01, activation_function=activ, optimizer='sgd', hidden_layers= 3, neurons_per_layer=[128, 64, 32], batch_size=32, epochs=50)\n",
    "#     mlp_activ.fit(X_train, y_train, early_stopping=True, patience=5)\n",
    "\n",
    "#     test_activations, _ = mlp_activ.forward(X_test)\n",
    "#     test_loss = mlp_activ.compute_loss(test_activations[-1], y_test)\n",
    "#     test_accuracy = mlp_activ.compute_accuracy(test_activations[-1], y_test)\n",
    "#     test_precision, test_recall, test_f1 = mlp_activ.compute_classification_metrics(y_test, mlp_activ.predict(X_test))\n",
    "#     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "#     print(f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}\")\n",
    "\n",
    "#     train_activations, _ = mlp_activ.forward(X_train)\n",
    "#     train_loss = mlp_activ.compute_loss(train_activations[-1], y_train)\n",
    "#     train_accuracy = mlp_activ.compute_accuracy(train_activations[-1], y_train)\n",
    "#     train_precision, train_recall, train_f1 = mlp_activ.compute_classification_metrics(y_train, mlp_activ.predict(X_train))\n",
    "#     print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "#     print(f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "#     eval_activations, _ = mlp_activ.forward(X_eval)\n",
    "#     eval_loss = mlp_activ.compute_loss(eval_activations[-1], y_eval)\n",
    "#     eval_accuracy = mlp_activ.compute_accuracy(eval_activations[-1], y_eval)\n",
    "#     eval_precision, eval_recall, eval_f1 = mlp_activ.compute_classification_metrics(y_eval, mlp_activ.predict(X_eval))\n",
    "#     print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")\n",
    "#     print(f\"Eval Precision: {eval_precision:.4f}, Eval Recall: {eval_recall:.4f}, Eval F1: {eval_f1:.4f}\")\n",
    "\n",
    "#     output_txt_file.write(f\"Activation Function: {activ}\\n\")\n",
    "#     output_txt_file.write(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "#     output_txt_file.write(f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}\\n\\n\")\n",
    "\n",
    "#     output_txt_file.write(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\\n\")\n",
    "#     output_txt_file.write(f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\\n\\n\")\n",
    "\n",
    "#     output_txt_file.write(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\\n\")\n",
    "#     output_txt_file.write(f\"Eval Precision: {eval_precision:.4f}, Eval Recall: {eval_recall:.4f}, Eval F1: {eval_f1:.4f}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Learning Rate:\n",
    "# lr_list = [0.001, 0.01, 0.1, 0.5]\n",
    "# output_txt_file = open(\"C:/Users/raaga/OneDrive/Desktop/IIIT-H/3-1/SMAI/smai-m24-assignments-rraagav/assignments/3/figures/vary_alpha.txt\", \"w\")\n",
    "\n",
    "# input_size = X_train.shape[1]\n",
    "# output_size = len(np.unique(y_train))\n",
    "\n",
    "# mlp_a = MLP_Classifier(input_size, output_size)\n",
    "\n",
    "# for a in lr_list:\n",
    "#     mlp_a.set_params(alpha=a, activation_function='tanh', optimizer='sgd', hidden_layers= 3, neurons_per_layer=[128, 64, 32], batch_size=32, epochs=50)\n",
    "#     mlp_a.fit(X_train, y_train, early_stopping=True, patience=5)\n",
    "\n",
    "#     test_activations, _ = mlp_a.forward(X_test)\n",
    "#     test_loss = mlp_a.compute_loss(test_activations[-1], y_test)\n",
    "#     test_accuracy = mlp_a.compute_accuracy(test_activations[-1], y_test)\n",
    "#     test_precision, test_recall, test_f1 = mlp_a.compute_classification_metrics(y_test, mlp_a.predict(X_test))\n",
    "#     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "#     print(f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}\")\n",
    "\n",
    "#     train_activations, _ = mlp_a.forward(X_train)\n",
    "#     train_loss = mlp_a.compute_loss(train_activations[-1], y_train)\n",
    "#     train_accuracy = mlp_a.compute_accuracy(train_activations[-1], y_train)\n",
    "#     train_precision, train_recall, train_f1 = mlp_a.compute_classification_metrics(y_train, mlp_a.predict(X_train))\n",
    "#     print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "#     print(f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "#     eval_activations, _ = mlp_a.forward(X_eval)\n",
    "#     eval_loss = mlp_a.compute_loss(eval_activations[-1], y_eval)\n",
    "#     eval_accuracy = mlp_a.compute_accuracy(eval_activations[-1], y_eval)\n",
    "#     eval_precision, eval_recall, eval_f1 = mlp_a.compute_classification_metrics(y_eval, mlp_a.predict(X_eval))\n",
    "#     print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")\n",
    "#     print(f\"Eval Precision: {eval_precision:.4f}, Eval Recall: {eval_recall:.4f}, Eval F1: {eval_f1:.4f}\")\n",
    "\n",
    "#     output_txt_file.write(f\"Learning Rate: {a}\\n\")\n",
    "#     output_txt_file.write(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "#     output_txt_file.write(f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}\\n\\n\")\n",
    "\n",
    "#     output_txt_file.write(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\\n\")\n",
    "#     output_txt_file.write(f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\\n\\n\")\n",
    "\n",
    "#     output_txt_file.write(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\\n\")\n",
    "#     output_txt_file.write(f\"Eval Precision: {eval_precision:.4f}, Eval Recall: {eval_recall:.4f}, Eval F1: {eval_f1:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Batch Size\n",
    "\n",
    "# batch_size = [16, 32, 64, 128]\n",
    "# output_txt_file = open(\"C:/Users/raaga/OneDrive/Desktop/IIIT-H/3-1/SMAI/smai-m24-assignments-rraagav/assignments/3/figures/vary_batch_size.txt\", \"w\")\n",
    "\n",
    "# input_size = X_train.shape[1]\n",
    "# output_size = len(np.unique(y_train))\n",
    "\n",
    "# mlp_bs = MLP_Classifier(input_size, output_size)\n",
    "\n",
    "# for bs in batch_size:\n",
    "#     mlp_bs.set_params(alpha=0.01, activation_function='tanh', optimizer='sgd', hidden_layers= 3, neurons_per_layer=[128, 64, 32], batch_size=bs, epochs=50)\n",
    "#     mlp_bs.fit(X_train, y_train, early_stopping=True, patience=5)\n",
    "\n",
    "#     test_activations, _ = mlp_bs.forward(X_test)\n",
    "#     test_loss = mlp_bs.compute_loss(test_activations[-1], y_test)\n",
    "#     test_accuracy = mlp_bs.compute_accuracy(test_activations[-1], y_test)\n",
    "#     test_precision, test_recall, test_f1 = mlp_bs.compute_classification_metrics(y_test, mlp_bs.predict(X_test))\n",
    "#     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "#     print(f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}\")\n",
    "\n",
    "#     train_activations, _ = mlp_bs.forward(X_train)\n",
    "#     train_loss = mlp_bs.compute_loss(train_activations[-1], y_train)\n",
    "#     train_accuracy = mlp_bs.compute_accuracy(train_activations[-1], y_train)\n",
    "#     train_precision, train_recall, train_f1 = mlp_bs.compute_classification_metrics(y_train, mlp_bs.predict(X_train))\n",
    "#     print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "#     print(f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "#     eval_activations, _ = mlp_bs.forward(X_eval)\n",
    "#     eval_loss = mlp_bs.compute_loss(eval_activations[-1], y_eval)\n",
    "#     eval_accuracy = mlp_bs.compute_accuracy(eval_activations[-1], y_eval)\n",
    "#     eval_precision, eval_recall, eval_f1 = mlp_bs.compute_classification_metrics(y_eval, mlp_bs.predict(X_eval))\n",
    "#     print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")\n",
    "#     print(f\"Eval Precision: {eval_precision:.4f}, Eval Recall: {eval_recall:.4f}, Eval F1: {eval_f1:.4f}\")\n",
    "\n",
    "#     output_txt_file.write(f\"Batch Size: {bs}\\n\")\n",
    "#     output_txt_file.write(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "#     output_txt_file.write(f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}\\n\\n\")\n",
    "\n",
    "#     output_txt_file.write(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\\n\")\n",
    "#     output_txt_file.write(f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\\n\\n\")\n",
    "\n",
    "#     output_txt_file.write(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\\n\")\n",
    "#     output_txt_file.write(f\"Eval Precision: {eval_precision:.4f}, Eval Recall: {eval_recall:.4f}, Eval F1: {eval_f1:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
